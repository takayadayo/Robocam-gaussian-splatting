#!/usr/bin/env python3
"""
Hand-Eye Calibration and 3D Reconstruction System
ãƒ­ãƒœãƒƒãƒˆã‚¢ãƒ¼ãƒ ã®ãƒãƒ¼ã‚ºæƒ…å ±ã¨ChArUcoãƒãƒ¼ã‚«ãƒ¼ç”»åƒã‚’ç”¨ã„ãŸé«˜ç²¾åº¦3Då†æ§‹æˆ
"""

import numpy as np
import cv2
import json
import argparse
from pathlib import Path
from scipy.spatial.transform import Rotation
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from typing import List, Dict, Tuple, Optional
import open3d as o3d


class HandEyeCalibration:
    """Hand-Eyeã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, aruco_dict_type=cv2.aruco.DICT_4X4_50, 
                 marker_size_mm=24.0, squares_x=7, squares_y=5):
        """
        Args:
            aruco_dict_type: ArUcoè¾æ›¸ã‚¿ã‚¤ãƒ—
            marker_size_mm: ãƒãƒ¼ã‚«ãƒ¼ã‚µã‚¤ã‚ºï¼ˆmmï¼‰
            squares_x, squares_y: ãƒã‚§ã‚¹ãƒœãƒ¼ãƒ‰ã®å†…éƒ¨ã‚³ãƒ¼ãƒŠãƒ¼æ•°
        """
        self.aruco_dict = cv2.aruco.getPredefinedDictionary(aruco_dict_type)
        self.board = cv2.aruco.CharucoBoard(
            (squares_x, squares_y),
            squareLength=marker_size_mm * 1.5 / 1000.0,  # må˜ä½
            markerLength=marker_size_mm / 1000.0,
            dictionary=self.aruco_dict
        )
        self.detector_params = cv2.aruco.DetectorParameters()
        
        # ã‚«ãƒ¡ãƒ©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.camera_matrix = None
        self.dist_coeffs = None
        
        # Hand-Eyeå¤‰æ›
        self.R_cam2gripper = None  # ã‚«ãƒ¡ãƒ©â†’ã‚°ãƒªãƒƒãƒ‘ãƒ¼å›è»¢
        self.t_cam2gripper = None  # ã‚«ãƒ¡ãƒ©â†’ã‚°ãƒªãƒƒãƒ‘ãƒ¼ä¸¦é€²
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
        self.captures_data = []
        self.valid_captures = []
        
    def load_captures(self, json_path: str, image_dir: str):
        """ã‚­ãƒ£ãƒ—ãƒãƒ£ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        print(f"ğŸ“‚ Loading captures from {json_path}")
        with open(json_path, 'r') as f:
            data = json.load(f)
        
        self.captures_data = []
        image_dir = Path(image_dir)
        
        for cap in data['captures']:
            img_path = image_dir / cap['image_file']
            if img_path.exists():
                self.captures_data.append({
                    'image_path': str(img_path),
                    'pose': cap['pose'],
                    'timestamp': cap['timestamp']
                })
        
        print(f"âœ… Loaded {len(self.captures_data)} captures")
        
    def detect_charuco_corners(self, image_path: str) -> Optional[Tuple]:
        """ChArUcoã‚³ãƒ¼ãƒŠãƒ¼ã®æ¤œå‡º"""
        img = cv2.imread(image_path)
        if img is None:
            return None
            
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        
        # ArUcoãƒãƒ¼ã‚«ãƒ¼æ¤œå‡º
        detector = cv2.aruco.ArucoDetector(self.aruco_dict, self.detector_params)
        corners, ids, rejected = detector.detectMarkers(gray)
        
        if ids is None or len(ids) < 4:
            return None
        
        # ChArUcoã‚³ãƒ¼ãƒŠãƒ¼è£œé–“
        num_corners, charuco_corners, charuco_ids = cv2.aruco.interpolateCornersCharuco(
            corners, ids, gray, self.board
        )
        
        if num_corners < 8:
            return None
            
        return charuco_corners, charuco_ids, img, corners, ids
    
    def calibrate_camera(self):
        """ã‚«ãƒ¡ãƒ©å†…éƒ¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        print("\nğŸ¯ Camera Intrinsic Calibration")
        
        all_corners = []
        all_ids = []
        img_size = None
        
        for i, cap in enumerate(self.captures_data):
            result = self.detect_charuco_corners(cap['image_path'])
            if result is None:
                continue
                
            charuco_corners, charuco_ids, img, _, _ = result
            all_corners.append(charuco_corners)
            all_ids.append(charuco_ids)
            
            if img_size is None:
                img_size = img.shape[:2][::-1]
            
            self.valid_captures.append(cap)
            
            if i < 3:  # æœ€åˆã®3æšã ã‘è¡¨ç¤º
                self._visualize_detection(img, charuco_corners, charuco_ids, i)
        
        print(f"âœ… Valid captures: {len(all_corners)}/{len(self.captures_data)}")
        
        if len(all_corners) < 5:
            raise ValueError("æœ‰åŠ¹ãªã‚­ãƒ£ãƒ—ãƒãƒ£ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
        
        # ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
        ret, self.camera_matrix, self.dist_coeffs, rvecs, tvecs = \
            cv2.aruco.calibrateCameraCharuco(
                all_corners, all_ids, self.board, img_size, None, None
            )
        
        print(f"ğŸ“ Reprojection Error: {ret:.4f} pixels")
        print(f"ğŸ“· Camera Matrix:\n{self.camera_matrix}")
        print(f"ğŸ“Š Distortion Coeffs: {self.dist_coeffs.ravel()}")
        
        return rvecs, tvecs
    
    def _visualize_detection(self, img, corners, ids, idx):
        """æ¤œå‡ºçµæœã®å¯è¦–åŒ–"""
        vis_img = img.copy()
        cv2.aruco.drawDetectedCornersCharuco(vis_img, corners, ids)
        
        plt.figure(figsize=(10, 6))
        plt.imshow(cv2.cvtColor(vis_img, cv2.COLOR_BGR2RGB))
        plt.title(f'ChArUco Detection - Image {idx+1}')
        plt.axis('off')
        plt.tight_layout()
        plt.show()
    
    def estimate_hand_eye_transformation(self):
        """Hand-Eyeå¤‰æ›è¡Œåˆ—ã®æ¨å®šï¼ˆEye-in-Handæ§‹æˆï¼‰"""
        print("\nğŸ¤– Hand-Eye Transformation Estimation")
        
        R_gripper2base_list = []
        t_gripper2base_list = []
        R_target2cam_list = []
        t_target2cam_list = []
        
        for cap in self.valid_captures:
            # ã‚°ãƒªãƒƒãƒ‘ãƒ¼â†’ãƒ™ãƒ¼ã‚¹å¤‰æ›ï¼ˆãƒ­ãƒœãƒƒãƒˆã‚¢ãƒ¼ãƒ ã®ãƒãƒ¼ã‚ºï¼‰
            pose = cap['pose']
            R_g2b = self._euler_to_rotation_matrix(
                pose['rx_deg'], pose['ry_deg'], pose['rz_deg']
            )
            t_g2b = np.array([pose['x_mm'], pose['y_mm'], pose['z_mm']]) / 1000.0
            
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆChArUcoãƒœãƒ¼ãƒ‰ï¼‰â†’ã‚«ãƒ¡ãƒ©å¤‰æ›
            result = self.detect_charuco_corners(cap['image_path'])
            if result is None:
                continue
            
            charuco_corners, charuco_ids, _, _, _ = result
            
            ret, rvec, tvec = cv2.aruco.estimatePoseCharucoBoard(
                charuco_corners, charuco_ids, self.board,
                self.camera_matrix, self.dist_coeffs, None, None
            )
            
            if not ret:
                continue
            
            R_t2c, _ = cv2.Rodrigues(rvec)
            t_t2c = tvec.ravel()
            
            R_gripper2base_list.append(R_g2b)
            t_gripper2base_list.append(t_g2b)
            R_target2cam_list.append(R_t2c)
            t_target2cam_list.append(t_t2c)
        
        print(f"âœ… Valid poses for Hand-Eye: {len(R_gripper2base_list)}")
        
        # OpenCVã®Hand-Eyeã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆEye-in-Handï¼‰
        R_cam2gripper, t_cam2gripper = cv2.calibrateHandEye(
            R_gripper2base_list,
            t_gripper2base_list,
            R_target2cam_list,
            t_target2cam_list,
            method=cv2.CALIB_HAND_EYE_TSAI
        )
        
        self.R_cam2gripper = R_cam2gripper
        self.t_cam2gripper = t_cam2gripper.ravel()
        
        print(f"ğŸ”— Hand-Eye Rotation:\n{self.R_cam2gripper}")
        print(f"ğŸ”— Hand-Eye Translation: {self.t_cam2gripper}")
        
    def compute_camera_poses(self) -> List[Dict]:
        """å„ç”»åƒã®ã‚«ãƒ¡ãƒ©ãƒãƒ¼ã‚ºã‚’ç®—å‡ºï¼ˆHand-Eyeå¤‰æ›ã‚’ä½¿ç”¨ï¼‰"""
        print("\nğŸ“¸ Computing Camera Poses")
        
        camera_poses = []
        
        for i, cap in enumerate(self.valid_captures):
            # ã‚°ãƒªãƒƒãƒ‘ãƒ¼â†’ãƒ™ãƒ¼ã‚¹å¤‰æ›
            pose = cap['pose']
            R_g2b = self._euler_to_rotation_matrix(
                pose['rx_deg'], pose['ry_deg'], pose['rz_deg']
            )
            t_g2b = np.array([pose['x_mm'], pose['y_mm'], pose['z_mm']]) / 1000.0
            
            # ã‚«ãƒ¡ãƒ©â†’ãƒ™ãƒ¼ã‚¹å¤‰æ› = (ã‚°ãƒªãƒƒãƒ‘ãƒ¼â†’ãƒ™ãƒ¼ã‚¹) * (ã‚«ãƒ¡ãƒ©â†’ã‚°ãƒªãƒƒãƒ‘ãƒ¼)
            R_cam2base = R_g2b @ self.R_cam2gripper
            t_cam2base = R_g2b @ self.t_cam2gripper + t_g2b
            
            # ãƒ™ãƒ¼ã‚¹â†’ã‚«ãƒ¡ãƒ©å¤‰æ›ï¼ˆã‚«ãƒ¡ãƒ©ãƒãƒ¼ã‚ºï¼‰
            R_base2cam = R_cam2base.T
            t_base2cam = -R_base2cam @ t_cam2base
            
            camera_poses.append({
                'image_path': cap['image_path'],
                'R': R_base2cam,
                't': t_base2cam,
                'timestamp': cap['timestamp']
            })
            
            if i < 3:
                print(f"  Camera {i+1}: t={t_base2cam}")
        
        return camera_poses
    
    def _euler_to_rotation_matrix(self, rx_deg, ry_deg, rz_deg):
        """ã‚ªã‚¤ãƒ©ãƒ¼è§’ã‹ã‚‰å›è»¢è¡Œåˆ—ã¸å¤‰æ›ï¼ˆZYXé †ï¼‰"""
        r = Rotation.from_euler('ZYX', [rz_deg, ry_deg, rx_deg], degrees=True)
        return r.as_matrix()
    
    def visualize_camera_poses(self, camera_poses: List[Dict]):
        """ã‚«ãƒ¡ãƒ©ãƒãƒ¼ã‚ºã®3Då¯è¦–åŒ–"""
        print("\nğŸ¨ Visualizing Camera Poses")
        
        fig = plt.figure(figsize=(12, 10))
        ax = fig.add_subplot(111, projection='3d')
        
        # ã‚«ãƒ¡ãƒ©ä½ç½®
        positions = np.array([pose['t'] for pose in camera_poses])
        ax.scatter(positions[:, 0], positions[:, 1], positions[:, 2],
                  c='royalblue', s=100, marker='o', label='Camera Centers')
        
        # ã‚«ãƒ¡ãƒ©ã®å‘ãï¼ˆå…‰è»¸ï¼‰
        for i, pose in enumerate(camera_poses):
            # Zè»¸æ–¹å‘ï¼ˆã‚«ãƒ¡ãƒ©ã®å‰æ–¹ï¼‰
            direction = pose['R'] @ np.array([0, 0, 0.1])
            ax.quiver(pose['t'][0], pose['t'][1], pose['t'][2],
                     direction[0], direction[1], direction[2],
                     color='red', arrow_length_ratio=0.3, linewidth=2)
        
        # åŸç‚¹ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰
        ax.scatter([0], [0], [0], c='gold', s=200, marker='*', 
                  label='World Origin', edgecolors='black', linewidths=2)
        
        ax.set_xlabel('X (m)', fontsize=12)
        ax.set_ylabel('Y (m)', fontsize=12)
        ax.set_zlabel('Z (m)', fontsize=12)
        ax.set_title('Camera Poses in World Frame', fontsize=14, fontweight='bold')
        ax.legend(fontsize=10)
        ax.grid(True, alpha=0.3)
        
        # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã®èª¿æ•´
        max_range = np.array([
            positions[:, 0].max() - positions[:, 0].min(),
            positions[:, 1].max() - positions[:, 1].min(),
            positions[:, 2].max() - positions[:, 2].min()
        ]).max() / 2.0
        
        mid_x = (positions[:, 0].max() + positions[:, 0].min()) * 0.5
        mid_y = (positions[:, 1].max() + positions[:, 1].min()) * 0.5
        mid_z = (positions[:, 2].max() + positions[:, 2].min()) * 0.5
        
        ax.set_xlim(mid_x - max_range, mid_x + max_range)
        ax.set_ylim(mid_y - max_range, mid_y + max_range)
        ax.set_zlim(mid_z - max_range, mid_z + max_range)
        
        plt.tight_layout()
        plt.show()
    
    def export_colmap_format(self, camera_poses: List[Dict], output_path: str):
        """COLMAPå½¢å¼ã§ã‚«ãƒ¡ãƒ©ãƒãƒ¼ã‚ºã‚’å‡ºåŠ›"""
        print(f"\nğŸ’¾ Exporting to COLMAP format: {output_path}")
        
        with open(output_path, 'w') as f:
            f.write("# Image list with two lines of data per image:\n")
            f.write("#   IMAGE_ID, QW, QX, QY, QZ, TX, TY, TZ, CAMERA_ID, NAME\n")
            f.write("#   POINTS2D[] as (X, Y, POINT3D_ID)\n")
            
            for i, pose in enumerate(camera_poses):
                # å›è»¢è¡Œåˆ—â†’ã‚¯ã‚©ãƒ¼ã‚¿ãƒ‹ã‚ªãƒ³
                r = Rotation.from_matrix(pose['R'])
                quat = r.as_quat()  # [x, y, z, w]
                qw, qx, qy, qz = quat[3], quat[0], quat[1], quat[2]
                
                tx, ty, tz = pose['t']
                
                img_name = Path(pose['image_path']).name
                
                # IMAGE_ID, QW, QX, QY, QZ, TX, TY, TZ, CAMERA_ID, NAME
                f.write(f"{i+1} {qw} {qx} {qy} {qz} {tx} {ty} {tz} 1 {img_name}\n")
                f.write("\n")  # ç©ºã®2Dç‚¹ãƒªã‚¹ãƒˆ
        
        print(f"âœ… Exported {len(camera_poses)} camera poses")


class SfMReconstructor:
    """Structure from Motion 3Då†æ§‹æˆå™¨"""
    
    def __init__(self, camera_matrix, dist_coeffs):
        self.camera_matrix = camera_matrix
        self.dist_coeffs = dist_coeffs
        
        # SIFTç‰¹å¾´æŠ½å‡ºå™¨ï¼ˆé«˜å“è³ªè¨­å®šï¼‰
        self.sift = cv2.SIFT_create(
            nfeatures=5000,
            contrastThreshold=0.03,
            edgeThreshold=15,
            sigma=1.2
        )
        
        # FLANN ãƒãƒƒãƒãƒ£ãƒ¼
        FLANN_INDEX_KDTREE = 1
        index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
        search_params = dict(checks=100)
        self.matcher = cv2.FlannBasedMatcher(index_params, search_params)
        
        self.images_data = []
        self.point_cloud = []
        self.point_colors = []
        
    def load_images_with_poses(self, camera_poses: List[Dict]):
        """ç”»åƒã¨ãƒãƒ¼ã‚ºãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        print("\nğŸ–¼ï¸  Loading images with known poses")
        
        for pose in camera_poses:
            img = cv2.imread(pose['image_path'])
            if img is None:
                continue
            
            # ã‚¢ãƒ³ãƒ‡ã‚£ã‚¹ãƒˆãƒ¼ã‚·ãƒ§ãƒ³
            img_undist = cv2.undistort(img, self.camera_matrix, self.dist_coeffs)
            
            # ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«å¤‰æ›
            gray = cv2.cvtColor(img_undist, cv2.COLOR_BGR2GRAY)
            
            # ç‰¹å¾´ç‚¹æŠ½å‡º
            kp, desc = self.sift.detectAndCompute(gray, None)
            
            self.images_data.append({
                'image': img_undist,
                'gray': gray,
                'keypoints': kp,
                'descriptors': desc,
                'R': pose['R'],
                't': pose['t'],
                'P': self._compute_projection_matrix(pose['R'], pose['t']),
                'image_path': pose['image_path']
            })
        
        print(f"âœ… Loaded {len(self.images_data)} images")
        for i, data in enumerate(self.images_data[:3]):
            print(f"  Image {i+1}: {len(data['keypoints'])} keypoints")
    
    def _compute_projection_matrix(self, R, t):
        """æŠ•å½±è¡Œåˆ—ã®è¨ˆç®— P = K[R|t]"""
        Rt = np.hstack([R, t.reshape(3, 1)])
        P = self.camera_matrix @ Rt
        return P
    
    def match_features_robust(self, idx1: int, idx2: int) -> Tuple[np.ndarray, np.ndarray]:
        """ãƒ­ãƒã‚¹ãƒˆãªç‰¹å¾´ãƒãƒƒãƒãƒ³ã‚°"""
        desc1 = self.images_data[idx1]['descriptors']
        desc2 = self.images_data[idx2]['descriptors']
        
        if desc1 is None or desc2 is None:
            return np.array([]), np.array([])
        
        # KNN ãƒãƒƒãƒãƒ³ã‚°
        matches = self.matcher.knnMatch(desc1, desc2, k=2)
        
        # Lowe's ratio test
        good_matches = []
        for m_n in matches:
            if len(m_n) != 2:
                continue
            m, n = m_n
            if m.distance < 0.7 * n.distance:
                good_matches.append(m)
        
        if len(good_matches) < 20:
            return np.array([]), np.array([])
        
        # å¯¾å¿œç‚¹ã®å–å¾—
        pts1 = np.float32([self.images_data[idx1]['keypoints'][m.queryIdx].pt 
                          for m in good_matches])
        pts2 = np.float32([self.images_data[idx2]['keypoints'][m.trainIdx].pt 
                          for m in good_matches])
        
        # RANSAC ã«ã‚ˆã‚‹å¤–ã‚Œå€¤é™¤å»
        E, mask = cv2.findEssentialMat(
            pts1, pts2, self.camera_matrix,
            method=cv2.RANSAC, prob=0.999, threshold=1.0
        )
        
        if mask is None:
            return np.array([]), np.array([])
        
        pts1 = pts1[mask.ravel() == 1]
        pts2 = pts2[mask.ravel() == 1]
        
        return pts1, pts2
    
    def triangulate_points(self, idx1: int, idx2: int, pts1: np.ndarray, pts2: np.ndarray):
        """ä¸‰è§’æ¸¬é‡ã«ã‚ˆã‚‹3Dç‚¹ã®å¾©å…ƒ"""
        P1 = self.images_data[idx1]['P']
        P2 = self.images_data[idx2]['P']
        
        # ä¸‰è§’æ¸¬é‡
        points_4d = cv2.triangulatePoints(P1, P2, pts1.T, pts2.T)
        points_3d = points_4d[:3] / points_4d[3]
        points_3d = points_3d.T
        
        # ã‚«ãƒ¡ãƒ©ã®å‰æ–¹ã«ã‚ã‚‹ç‚¹ã®ã¿ä¿æŒï¼ˆãƒã‚§ã‚¤ãƒ©ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯ï¼‰
        valid_points = []
        valid_colors = []
        
        img1 = self.images_data[idx1]['image']
        
        for i, pt3d in enumerate(points_3d):
            # ä¸¡æ–¹ã®ã‚«ãƒ¡ãƒ©ã‹ã‚‰è¦‹ã¦å‰æ–¹ã«ã‚ã‚‹ã‹ç¢ºèª
            pt1_cam = self.images_data[idx1]['R'] @ pt3d + self.images_data[idx1]['t']
            pt2_cam = self.images_data[idx2]['R'] @ pt3d + self.images_data[idx2]['t']
            
            if pt1_cam[2] > 0 and pt2_cam[2] > 0:
                # å†æŠ•å½±èª¤å·®ãƒã‚§ãƒƒã‚¯
                reproj1 = P1 @ np.append(pt3d, 1)
                reproj1 = reproj1[:2] / reproj1[2]
                error1 = np.linalg.norm(reproj1 - pts1[i])
                
                reproj2 = P2 @ np.append(pt3d, 1)
                reproj2 = reproj2[:2] / reproj2[2]
                error2 = np.linalg.norm(reproj2 - pts2[i])
                
                if error1 < 3.0 and error2 < 3.0:  # å†æŠ•å½±èª¤å·®é–¾å€¤
                    valid_points.append(pt3d)
                    
                    # è‰²æƒ…å ±ã®å–å¾—
                    x, y = int(pts1[i][0]), int(pts1[i][1])
                    if 0 <= y < img1.shape[0] and 0 <= x < img1.shape[1]:
                        color = img1[y, x][::-1] / 255.0  # BGRâ†’RGB, æ­£è¦åŒ–
                        valid_colors.append(color)
                    else:
                        valid_colors.append([0.5, 0.5, 0.5])
        
        return np.array(valid_points), np.array(valid_colors)
    
    def reconstruct_3d(self):
        """3Då†æ§‹æˆã®å®Ÿè¡Œ"""
        print("\nğŸ—ï¸  3D Reconstruction")
        
        n_images = len(self.images_data)
        
        # å…¨ãƒšã‚¢é–“ã§ä¸‰è§’æ¸¬é‡
        for i in range(n_images):
            for j in range(i + 1, n_images):
                print(f"  Processing pair ({i+1}, {j+1})...")
                
                pts1, pts2 = self.match_features_robust(i, j)
                
                if len(pts1) < 20:
                    print(f"    âš ï¸  Insufficient matches: {len(pts1)}")
                    continue
                
                # ãƒãƒƒãƒãƒ³ã‚°å¯è¦–åŒ–ï¼ˆæœ€åˆã®ãƒšã‚¢ã®ã¿ï¼‰
                if i == 0 and j == 1:
                    self._visualize_matches(i, j, pts1, pts2)
                
                points_3d, colors = self.triangulate_points(i, j, pts1, pts2)
                
                if len(points_3d) > 0:
                    self.point_cloud.append(points_3d)
                    self.point_colors.append(colors)
                    print(f"    âœ… Triangulated {len(points_3d)} points")
        
        # ç‚¹ç¾¤ã®çµåˆ
        if len(self.point_cloud) > 0:
            self.point_cloud = np.vstack(self.point_cloud)
            self.point_colors = np.vstack(self.point_colors)
            
            # å¤–ã‚Œå€¤é™¤å»ï¼ˆçµ±è¨ˆçš„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰
            self.point_cloud, self.point_colors = self._remove_outliers(
                self.point_cloud, self.point_colors
            )
            
            print(f"\nâœ… Total reconstructed points: {len(self.point_cloud)}")
        else:
            print("\nâš ï¸  No points reconstructed")
    
    def _visualize_matches(self, idx1, idx2, pts1, pts2):
        """ç‰¹å¾´ãƒãƒƒãƒãƒ³ã‚°ã®å¯è¦–åŒ–"""
        img1 = self.images_data[idx1]['image']
        img2 = self.images_data[idx2]['image']
        
        # ç”»åƒã‚’æ¨ªã«ä¸¦ã¹ã‚‹
        h1, w1 = img1.shape[:2]
        h2, w2 = img2.shape[:2]
        h = max(h1, h2)
        
        vis = np.zeros((h, w1 + w2, 3), dtype=np.uint8)
        vis[:h1, :w1] = img1
        vis[:h2, w1:w1+w2] = img2
        
        # ãƒãƒƒãƒãƒ³ã‚°ã‚’æç”»ï¼ˆæœ€å¤§50å€‹ï¼‰
        n_show = min(50, len(pts1))
        for i in range(n_show):
            pt1 = tuple(pts1[i].astype(int))
            pt2 = tuple((pts2[i] + [w1, 0]).astype(int))
            
            color = tuple(np.random.randint(100, 255, 3).tolist())
            cv2.circle(vis, pt1, 3, color, -1)
            cv2.circle(vis, pt2, 3, color, -1)
            cv2.line(vis, pt1, pt2, color, 1)
        
        plt.figure(figsize=(16, 8))
        plt.imshow(cv2.cvtColor(vis, cv2.COLOR_BGR2RGB))
        plt.title(f'Feature Matches: Image {idx1+1} â†” Image {idx2+1} ({len(pts1)} matches)', 
                 fontsize=14)
        plt.axis('off')
        plt.tight_layout()
        plt.show()
    
    def _remove_outliers(self, points, colors, nb_neighbors=20, std_ratio=2.0):
        """çµ±è¨ˆçš„å¤–ã‚Œå€¤é™¤å»"""
        print("\nğŸ§¹ Removing outliers...")
        
        pcd = o3d.geometry.PointCloud()
        pcd.points = o3d.utility.Vector3dVector(points)
        
        # çµ±è¨ˆçš„å¤–ã‚Œå€¤é™¤å»
        cl, ind = pcd.remove_statistical_outlier(nb_neighbors=nb_neighbors,
                                                  std_ratio=std_ratio)
        
        points_clean = np.asarray(cl.points)
        colors_clean = colors[ind]
        
        print(f"  Removed {len(points) - len(points_clean)} outliers")
        print(f"  Remaining points: {len(points_clean)}")
        
        return points_clean, colors_clean
    
    def visualize_reconstruction(self):
        """3Då†æ§‹æˆçµæœã®å¯è¦–åŒ–"""
        if len(self.point_cloud) == 0:
            print("âš ï¸  No points to visualize")
            return
        
        print("\nğŸ¨ Visualizing 3D Reconstruction")
        
        # Open3Dã§å¯è¦–åŒ–
        pcd = o3d.geometry.PointCloud()
        pcd.points = o3d.utility.Vector3dVector(self.point_cloud)
        pcd.colors = o3d.utility.Vector3dVector(self.point_colors)
        
        # ã‚«ãƒ¡ãƒ©ãƒ•ãƒ©ã‚¹ã‚¿ãƒ ã®è¿½åŠ 
        frustums = []
        for i, data in enumerate(self.images_data):
            frustum = self._create_camera_frustum(data['R'], data['t'], scale=0.05)
            frustums.append(frustum)
        
        # åº§æ¨™è»¸
        coord_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(
            size=0.1, origin=[0, 0, 0]
        )
        
        # å¯è¦–åŒ–
        o3d.visualization.draw_geometries(
            [pcd, coord_frame] + frustums,
            window_name="3D Reconstruction",
            width=1200, height=900,
            point_show_normal=False
        )
    
    def _create_camera_frustum(self, R, t, scale=0.05):
        """ã‚«ãƒ¡ãƒ©ãƒ•ãƒ©ã‚¹ã‚¿ãƒ ã®ä½œæˆ"""
        # ã‚«ãƒ¡ãƒ©ä¸­å¿ƒ
        center = -R.T @ t
        
        # ãƒ•ãƒ©ã‚¹ã‚¿ãƒ ã®é ‚ç‚¹ï¼ˆã‚«ãƒ¡ãƒ©åº§æ¨™ç³»ï¼‰
        pts_cam = np.array([
            [0, 0, 0],
            [-1, -1, 2],
            [1, -1, 2],
            [1, 1, 2],
            [-1, 1, 2]
        ]) * scale
        
        # ãƒ¯ãƒ¼ãƒ«ãƒ‰åº§æ¨™ç³»ã¸å¤‰æ›
        pts_world = (R.T @ pts_cam.T).T + center
        
        # LineSetä½œæˆ
        lines = [[0, 1], [0, 2], [0, 3], [0, 4],
                [1, 2], [2, 3], [3, 4], [4, 1]]
        colors = [[1, 0, 0] for _ in range(len(lines))]
        
        line_set = o3d.geometry.LineSet()
        line_set.points = o3d.utility.Vector3dVector(pts_world)
        line_set.lines = o3d.utility.Vector2iVector(lines)
        line_set.colors = o3d.utility.Vector3dVector(colors)
        
        return line_set


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(
        description='Hand-Eye Calibration and 3D Reconstruction'
    )
    parser.add_argument('--json', required=True, help='Path to captures JSON file')
    parser.add_argument('--images', required=True, help='Path to images directory')
    parser.add_argument('--marker-size', type=float, default=24.0,
                       help='ArUco marker size in mm (default: 50.0)')
    parser.add_argument('--squares-x', type=int, default=7,
                       help='Number of squares in X direction (default: 5)')
    parser.add_argument('--squares-y', type=int, default=5,
                       help='Number of squares in Y direction (default: 7)')
    parser.add_argument('--output-colmap', default='images.txt',
                       help='Output path for COLMAP format (default: images.txt)')
    
    args = parser.parse_args()
    
    print("=" * 30)
    print("  Hand-Eye Calibration & 3D Reconstruction System")
    print("=" * 30)
    
    # ===============================================
    # Phase 1: Hand-Eye Calibration
    # ===============================================
    print("\n" + "=" * 30)
    print("  PHASE 1: Hand-Eye Calibration")
    print("=" * 30)
    
    calib = HandEyeCalibration(
        aruco_dict_type=cv2.aruco.DICT_4X4_50,
        marker_size_mm=args.marker_size,
        squares_x=args.squares_x,
        squares_y=args.squares_y
    )
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    calib.load_captures(args.json, args.images)
    
    # ã‚«ãƒ¡ãƒ©ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    rvecs, tvecs = calib.calibrate_camera()
    
    # Hand-Eyeå¤‰æ›æ¨å®š
    calib.estimate_hand_eye_transformation()
    
    # ã‚«ãƒ¡ãƒ©ãƒãƒ¼ã‚ºç®—å‡º
    camera_poses = calib.compute_camera_poses()
    
    # ã‚«ãƒ¡ãƒ©ãƒãƒ¼ã‚ºå¯è¦–åŒ–
    calib.visualize_camera_poses(camera_poses)
    
    # COLMAPå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    calib.export_colmap_format(camera_poses, args.output_colmap)
    
    # ===============================================
    # Phase 2: 3D Reconstruction
    # ===============================================
    print("\n" + "=" * 30)
    print("  PHASE 2: 3D Reconstruction with Known Poses")
    print("=" * 30)
    
    reconstructor = SfMReconstructor(
        calib.camera_matrix,
        calib.dist_coeffs
    )
    
    # ç”»åƒã¨æ—¢çŸ¥ãƒãƒ¼ã‚ºã®èª­ã¿è¾¼ã¿
    reconstructor.load_images_with_poses(camera_poses)
    
    # 3Då†æ§‹æˆå®Ÿè¡Œ
    reconstructor.reconstruct_3d()
    
    # çµæœã®å¯è¦–åŒ–
    reconstructor.visualize_reconstruction()
    
    # ===============================================
    # Summary
    # ===============================================
    print("\n" + "=" * 30)
    print("  SUMMARY")
    print("=" * 30)
    print(f"âœ… Camera Intrinsics:")
    print(f"   fx={calib.camera_matrix[0,0]:.2f}, fy={calib.camera_matrix[1,1]:.2f}")
    print(f"   cx={calib.camera_matrix[0,2]:.2f}, cy={calib.camera_matrix[1,2]:.2f}")
    print(f"\nâœ… Hand-Eye Transform (Camera â†’ Gripper):")
    print(f"   Translation: {calib.t_cam2gripper}")
    print(f"\nâœ… Camera Poses: {len(camera_poses)} poses computed")
    print(f"   Exported to: {args.output_colmap}")
    print(f"\nâœ… 3D Reconstruction: {len(reconstructor.point_cloud)} points")
    print("=" * 30)
    print("\nğŸ‰ Processing Complete!")


if __name__ == "__main__":
    main()